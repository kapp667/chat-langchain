# Benchmark HERO vs PRAGMATIC : M√©thodologie Compl√®te

**Date:** 3 octobre 2025
**Objectif:** Comparer qualit√© absolue Sonnet 4.5 (HERO) vs Llama 3.3 70B + PNL (PRAGMATIC) sur 6 questions gradu√©es
**Auteur:** St√©phane Wootha Richard

---

## R√©sum√© Ex√©cutif

Ce benchmark d√©partage d√©finitivement les deux candidats finaux pour le serveur MCP :
- **HERO** : Claude Sonnet 4.5 (champion qualit√©, mais co√ªt/latence √©lev√©s)
- **PRAGMATIC** : Llama 3.3 70B + PNL (compromis optimal vitesse/qualit√©/co√ªt)

**Hypoth√®se √† tester :**
> Llama 3.3 70B avec PNL anti-hallucination peut-il fournir une qualit√© √©quivalente √† Sonnet 4.5 malgr√© un co√ªt 153x inf√©rieur et une latence 6x plus rapide ?

**M√©thodologie :** 6 questions gradu√©es (triviale ‚Üí ultra-complexe) test√©es sur les deux mod√®les, analyse qualitative par sous-agent sp√©cialis√©.

---

## 1. Design des Questions

### 1.1 Principe de Gradation

**Objectif :** Tester les mod√®les sur toute la gamme de complexit√© rencontr√©e en usage r√©el MCP.

**Strat√©gie de gradation :**

| Niveau | Complexit√© | Type question | Longueur attendue | Composants requis |
|--------|------------|---------------|-------------------|-------------------|
| **Q1** | Triviale | API lookup | 1-5 lignes | 1 classe |
| **Q2** | Simple | Configuration | 5-15 lignes | 2-3 param√®tres |
| **Q3** | Mod√©r√©e | Int√©gration | 20-40 lignes | 3-5 composants |
| **Q4** | Mod√©r√©e-complexe | Debugging | 30-60 lignes | Troubleshooting + solutions |
| **Q5** | Complexe | Architecture multi-√©tapes | 50-120 lignes | Graph + nodes + routing |
| **Q6** | Ultra-complexe | Graph avanc√© + backtracking | 100-250 lignes | State management + time travel |

**Principe questions na√Øves :**
- ‚úÖ Focus besoins utilisateur (pas jargon technique)
- ‚úÖ Pas d'induction de r√©ponse (question ouverte)
- ‚úÖ Sc√©narios r√©els d'usage d√©veloppeur MCP

### 1.2 Questions D√©taill√©es

#### Q1 (Triviale) : API Lookup Simple

**Question :**
> "I need to save conversation history to PostgreSQL. Which class should I use?"

**Objectif :** Tester rapidit√© de r√©ponse factuelle (1 classe √† identifier)

**R√©ponse attendue :**
- AsyncPostgresSaver ou PostgresSaver
- 1-5 lignes maximum
- Import statement + explication 1 phrase

**Crit√®res √©valuation :**
- ‚úÖ Classe correcte identifi√©e
- ‚úÖ Concision (pas de d√©tails inutiles)
- ‚úÖ Latence <10s (question triviale)

---

#### Q2 (Simple) : Configuration Basique

**Question :**
> "How do I configure LangGraph to use my own OpenAI API key and custom temperature?"

**Objectif :** Tester explication configuration simple (2 param√®tres)

**R√©ponse attendue :**
- Configuration OPENAI_API_KEY (env var ou direct)
- Param√®tre temperature dans ChatOpenAI
- 5-15 lignes (exemple code minimal)

**Crit√®res √©valuation :**
- ‚úÖ OPENAI_API_KEY correctement expliqu√©
- ‚úÖ temperature parameter mentionn√©
- ‚úÖ Code exemple fonctionnel
- ‚úÖ Latence <15s

---

#### Q3 (Mod√©r√©e) : Int√©gration Composants

**Question :**
> "I want to build a chatbot that remembers previous messages across sessions. What components do I need and how do I connect them?"

**Objectif :** Tester capacit√© d'int√©gration multi-composants

**R√©ponse attendue :**
- Checkpointer (PostgresSaver ou MemorySaver)
- thread_id pour sessions
- StateGraph compilation
- 20-40 lignes (architecture + code exemple)

**Crit√®res √©valuation :**
- ‚úÖ Checkpointer mentionn√©
- ‚úÖ thread_id pour persistence expliqu√©
- ‚úÖ Graph compilation correct
- ‚úÖ Code exemple complet et fonctionnel
- ‚úÖ Latence <20s

---

#### Q4 (Mod√©r√©e-Complexe) : Debugging & R√©silience

**Question :**
> "My LangGraph agent keeps timing out after 30 seconds on complex questions. How can I debug this and make it more resilient?"

**Objectif :** Tester troubleshooting pratique + solutions multiples

**R√©ponse attendue :**
- Diagnostic (recursion_limit, timeout config, streaming)
- Solutions (augmenter limits, streaming pour feedback, error handling)
- 30-60 lignes (diagnostic + 3-5 solutions)

**Crit√®res √©valuation :**
- ‚úÖ Diagnostic causes timeout (recursion_limit, long documents)
- ‚úÖ Solutions pratiques (config, streaming, error recovery)
- ‚úÖ Code exemples fonctionnels
- ‚úÖ Latence <30s

---

#### Q5 (Complexe) : Architecture Multi-√âtapes RAG

**Question :**
> "I need to build a research assistant that: (1) breaks down complex questions into sub-questions, (2) searches documentation for each sub-question, (3) synthesizes findings. How should I structure this?"

**Objectif :** Tester architecture graph multi-nodes avec routing

**R√©ponse attendue :**
- StateGraph avec nodes (decompose, search, synthesize)
- Multi-query generation
- Conditional routing
- 50-120 lignes (architecture + code exemple complet)

**Crit√®res √©valuation :**
- ‚úÖ Architecture claire (3+ nodes identifi√©s)
- ‚úÖ Multi-query strategy expliqu√©e
- ‚úÖ Synthesis logic correcte
- ‚úÖ Code graph fonctionnel
- ‚úÖ Latence <45s (acceptable pour complexit√©)

---

#### Q6 (Ultra-Complexe) : Graph Avanc√© avec Backtracking

**Question :**
> "I want to create a planning agent that explores multiple solution paths, can backtrack when hitting dead ends, and maintains a tree of attempted strategies. How do I implement this with LangGraph?"

**Objectif :** Tester capacit√©s avanc√©es (state management, time travel, backtracking)

**R√©ponse attendue :**
- StateGraph avec state complexe (tree of attempts)
- Checkpoints pour time travel
- Conditional routing pour exploration paths
- Backtracking via checkpoint replay
- 100-250 lignes (architecture avanc√©e + code complet)

**Crit√®res √©valuation :**
- ‚úÖ State management arbre de d√©cisions
- ‚úÖ Checkpoints pour backtracking expliqu√©s
- ‚úÖ Time travel API utilis√©e correctement
- ‚úÖ Graph routing avanc√© (conditional edges)
- ‚úÖ Code fonctionnel et production-ready
- ‚úÖ Latence <90s (acceptable ultra-complexe)

---

## 2. Mod√®les Test√©s

### 2.1 HERO : Claude Sonnet 4.5

**Configuration :**
```python
{
    "id": "anthropic/claude-sonnet-4-5-20250929",
    "name": "Claude Sonnet 4.5 (HERO)",
    "description": "Excellence maximale (champion qualit√©)",
    "expected_speed": "slow (40-120s)",
    "expected_quality": "5/5"
}
```

**Caract√©ristiques :**
- ‚úÖ Champion absolu qualit√© (benchmarks pr√©c√©dents : 23.8K chars sur Q complexe)
- ‚úÖ 0% hallucinations (natif, sans PNL requis)
- ‚úÖ Exhaustivit√© maximale (citations, exemples code multiples)
- ‚ùå Co√ªt √©lev√© : $90/M tokens (input $15, output $75)
- ‚ùå Latence √©lev√©e : 60s moyenne (40-120s range)

**Attentes benchmark :**
- Qualit√© : 5/5 sur toutes questions
- Verbosit√© : Adaptative naturelle (mais tendance d√©tail excessif)
- Hallucinations : 0% (natif)
- Latence totale (6Q) : ~6-12 minutes

### 2.2 PRAGMATIC : Llama 3.3 70B + PNL

**Configuration :**
```python
{
    "id": "groq/llama-3.3-70b-versatile",
    "name": "Llama 3.3 70B + PNL (PRAGMATIC)",
    "description": "Compromis optimal vitesse/qualit√© (anti-hallucination PNL)",
    "expected_speed": "fast (8-15s)",
    "expected_quality": "4.6/5"
}
```

**Caract√©ristiques :**
- ‚úÖ Vitesse excellente : 9.73s moyenne (6x plus rapide que Sonnet)
- ‚úÖ Co√ªt ultra-comp√©titif : $0.59/M tokens (153x moins cher)
- ‚úÖ 0% hallucinations (PNL "Documentation Mirror Mode" valid√©)
- ‚úÖ Verbosit√© adaptative : 2.2K-6.2K chars selon complexit√©
- ‚ö†Ô∏è Qualit√© l√©g√®rement inf√©rieure : 4.6/5 (vs 5/5 Sonnet)
- ‚ö†Ô∏è Moins de citations que Sonnet (5 vs 25 moyenne)

**Wrapper PNL utilis√© :** `backend/groq_wrapper.py` (injection anti-hallucination)

**Attentes benchmark :**
- Qualit√© : 4.6/5 moyenne (acceptable si >4/5 sur Q complexes)
- Verbosit√© : Adaptative (concis sur Q1-Q2, d√©taill√© Q5-Q6)
- Hallucinations : 0% (PNL valid√©)
- Latence totale (6Q) : ~1.5-3 minutes

---

## 3. Infrastructure et Protocole

### 3.1 Infrastructure de Test

**Backend :**
- LangGraph 0.4.5 (local `langgraph dev`)
- Weaviate v4 Cloud (15,061 documents LangChain index√©s)
- Endpoint : http://localhost:2024

**Embeddings :**
- OpenAI text-embedding-3-small
- Context : 15,061 docs (guides, tutorials, API refs)

**Invocation :**
```python
POST http://localhost:2024/invoke
{
  "input": {
    "messages": [{"role": "user", "content": "{question}"}]
  },
  "config": {
    "configurable": {
      "model": "{model_id}"  # sonnet45 ou llama-3.3-70b-groq
    }
  }
}
```

### 3.2 Protocole d'Ex√©cution

**S√©quence :**
1. Lancer les deux benchmarks en parall√®le (scripts ind√©pendants)
   ```bash
   # Terminal 1
   poetry run python mcp_server/archive/benchmark_hero_vs_pragmatic.py --model llama-3.3-70b-groq

   # Terminal 2 (d√©lai 3s)
   poetry run python mcp_server/archive/benchmark_hero_vs_pragmatic.py --model sonnet45
   ```

2. Chaque benchmark ex√©cute 6 questions s√©quentiellement
   - D√©lai 2s entre questions (√©viter saturation endpoint)
   - Timeout 300s par question (5 min max)

3. Sauvegarder r√©sultats JSON
   - `mcp_server/results/hero_vs_pragmatic_llama-3.3-70b-groq_results.json`
   - `mcp_server/results/hero_vs_pragmatic_sonnet45_results.json`

### 3.3 M√©triques Collect√©es

**Par question :**
- `success` : bool (r√©ussite/√©chec)
- `elapsed_time` : float (secondes)
- `response_full` : str (r√©ponse compl√®te)
- `response_length` : int (chars)
- `chunk_count` : int (documents r√©cup√©r√©s)
- `error` : str | null (si √©chec)

**Statistiques globales :**
- `total_tests` : 6
- `successful_tests` : N/6
- `average_time` : float (latence moyenne)
- `average_response_length` : float (verbosit√© moyenne)

---

## 4. Crit√®res d'√âvaluation Qualitative

### 4.1 Grille d'Analyse (Sous-Agent)

**Crit√®res par question :**

| Crit√®re | Poids | Description |
|---------|-------|-------------|
| **Exactitude factuelle** | 30% | APIs/classes correctes, 0 hallucinations |
| **Compl√©tude** | 25% | Tous aspects question couverts |
| **Actionabilit√©** | 20% | Code exemples fonctionnels |
| **Clart√©** | 15% | Structure logique, explicatif |
| **Concision** | 10% | Pas de d√©tails inutiles (Q triviales/simples) |

**Score global :**
- 5/5 : Excellence (tous crit√®res remplis)
- 4/5 : Tr√®s bon (1 crit√®re partiel)
- 3/5 : Bon (2 crit√®res partiels)
- 2/5 : Moyen (crit√®res majeurs manquants)
- 1/5 : Insuffisant (r√©ponse incorrecte/incompl√®te)

### 4.2 Analyse Diff√©rentielle

**Questions cl√©s pour sous-agent :**

1. **Hallucinations :**
   - Llama 3.3 70B invente-t-il des m√©thodes/classes ? (PNL efficace ?)
   - Sonnet 4.5 reste-t-il factuel √† 100% ?

2. **Verbosit√© adaptative :**
   - Llama adapte-t-il sa longueur selon complexit√© question ?
   - Sonnet sur-d√©taille-t-il les questions simples (Q1-Q2) ?

3. **Qualit√© sur questions complexes (Q5-Q6) :**
   - Llama fournit-il architecture compl√®te et fonctionnelle ?
   - Gap Llama vs Sonnet sur ultra-complexe (Q6) est-il acceptable ?

4. **Actionabilit√© code :**
   - Code Llama est-il production-ready ?
   - Sonnet fournit-il plus d'exemples mais usage identique ?

5. **ROI qualit√©/latence :**
   - Si Llama 4.5/5 sur Q1-Q4 et 4/5 sur Q5-Q6, est-ce acceptable pour MCP ?
   - Gain latence 6x compense-t-il perte qualit√© -10% ?

---

## 5. Analyse Attendue (Hypoth√®ses)

### 5.1 Hypoth√®se HERO (Sonnet 4.5)

**Attendu :**
- ‚úÖ Qualit√© 5/5 sur toutes questions
- ‚úÖ Exhaustivit√© maximale (citations multiples, exemples code d√©taill√©s)
- ‚úÖ 0% hallucinations (natif)
- ‚ùå Verbosit√© excessive sur Q1-Q2 (over-engineering)
- ‚ùå Latence 40-120s par question (total 6-12 min)

**Profil :**
> Champion absolu qualit√©, mais inadapt√© √† usage MCP interactif (latence frustrante pour d√©veloppeur)

### 5.2 Hypoth√®se PRAGMATIC (Llama 3.3 70B + PNL)

**Attendu :**
- ‚úÖ Qualit√© 4.5-5/5 sur Q1-Q4 (triviales/simples/mod√©r√©es)
- ‚ö†Ô∏è Qualit√© 4-4.5/5 sur Q5-Q6 (complexes/ultra-complexes)
- ‚úÖ Verbosit√© adaptative : concis Q1-Q2, d√©taill√© Q5-Q6
- ‚úÖ 0% hallucinations (PNL efficace)
- ‚úÖ Latence 8-15s par question (total 1.5-3 min)

**Profil :**
> Compromis optimal pour MCP : qualit√© production-ready (>4/5) avec latence acceptable (<15s moyenne)

### 5.3 Hypoth√®se D√©cision Finale

**Si Llama ‚â•4/5 sur Q1-Q4 ET ‚â•3.5/5 sur Q5-Q6 :**
‚Üí **PRAGMATIC gagnant** (qualit√© acceptable, ROI latence/co√ªt sup√©rieur)

**Si Llama <4/5 sur Q5-Q6 (gap >20% vs Sonnet) :**
‚Üí **HERO gagnant** (qualit√© critique non n√©gociable pour questions complexes)

**Seuil d√©cisif :** Qualit√© Llama sur Q6 (ultra-complexe backtracking)
- Si ‚â•4/5 ‚Üí PRAGMATIC recommand√©
- Si <3.5/5 ‚Üí HERO n√©cessaire pour 10% questions ultra-complexes

---

## 6. Utilisation Sous-Agent Sp√©cialis√©

### 6.1 T√¢che Sous-Agent

**Agent s√©lectionn√© :** `general-purpose` ou `codebase-quality-analyzer`

**Prompt sous-agent :**
```
Analyse qualitative comparative HERO vs PRAGMATIC :

CONTEXTE :
- HERO = Claude Sonnet 4.5 (champion qualit√©, $90/M, 60s latence)
- PRAGMATIC = Llama 3.3 70B + PNL ($0.59/M, 10s latence)

T√ÇCHE :
1. Lire int√©gralement les deux fichiers JSON :
   - mcp_server/results/hero_vs_pragmatic_sonnet45_results.json
   - mcp_server/results/hero_vs_pragmatic_llama-3.3-70b-groq_results.json

2. Analyser qualit√© r√©ponses selon 5 crit√®res :
   - Exactitude factuelle (APIs correctes, 0 hallucinations)
   - Compl√©tude (tous aspects couverts)
   - Actionabilit√© (code fonctionnel)
   - Clart√© (structure logique)
   - Concision (Q1-Q2 pas over-engineering)

3. Scorer chaque question (1-5/5) pour chaque mod√®le

4. Identifier diff√©rences cl√©s :
   - Hallucinations (Llama invente-t-il m√©thodes ?)
   - Verbosit√© (Llama adapte-t-il selon complexit√© ?)
   - Gap Q5-Q6 (complexes) : acceptable (<20%) ?

5. RECOMMANDATION FINALE :
   - Si Llama ‚â•4/5 sur Q1-Q4 ET ‚â•3.5/5 sur Q5-Q6 ‚Üí PRAGMATIC
   - Si Llama <3.5/5 sur Q5-Q6 ‚Üí HERO

LIVRABLES :
- Tableau comparatif 6 questions √ó 5 crit√®res √ó 2 mod√®les
- Analyse qualitative diff√©rentielle (300 mots)
- Recommandation finale justifi√©e
```

### 6.2 Sauvegarde Analyse

**Fichier output sous-agent :**
- `ANALYSE_QUALITATIVE_HERO_VS_PRAGMATIC.md`

**Contenu attendu :**
1. Tableau scores (6Q √ó 5 crit√®res √ó 2 mod√®les)
2. Analyse diff√©rentielle (hallucinations, verbosit√©, gaps)
3. Recommandation finale (HERO ou PRAGMATIC)
4. Justification d√©cision (ROI qualit√©/latence/co√ªt)

---

## 7. Prochaines √âtapes

### 7.1 Ex√©cution Benchmarks

**Statut actuel :** En cours (lanc√©s en parall√®le)
- ‚úÖ Llama 3.3 70B : Benchmark lanc√© (PID 2deeb9)
- ‚úÖ Sonnet 4.5 : Benchmark lanc√© (PID 2b3340)

**Dur√©e estim√©e :**
- Llama : ~1.5-3 minutes (6Q √ó 15s moyenne)
- Sonnet : ~6-12 minutes (6Q √ó 60s moyenne)

**Surveillance :**
```bash
# Llama
tail -f /tmp/benchmark_hero_vs_pragmatic_llama.log

# Sonnet
tail -f /tmp/benchmark_hero_vs_pragmatic_sonnet.log
```

### 7.2 Analyse Post-Benchmark

1. ‚úÖ V√©rifier r√©sultats JSON complets (6/6 tests r√©ussis)
2. ‚úÖ Soumettre analyse qualitative au sous-agent
3. ‚úÖ Documenter recommandation finale
4. ‚è≥ Configurer serveur MCP avec mod√®le s√©lectionn√©

### 7.3 Crit√®res de Succ√®s

**Benchmarks r√©ussis si :**
- ‚úÖ 6/6 questions ex√©cut√©es sans erreur (les deux mod√®les)
- ‚úÖ R√©ponses compl√®tes (>500 chars minimum)
- ‚úÖ Latences dans limites attendues (Llama <20s, Sonnet <150s)

**Analyse qualitative r√©ussie si :**
- ‚úÖ Scores d√©taill√©s 6Q √ó 5 crit√®res √ó 2 mod√®les
- ‚úÖ Diff√©rences cl√©s identifi√©es (hallucinations, verbosit√©, gaps)
- ‚úÖ Recommandation finale claire et justifi√©e

---

## 8. R√©f√©rences

### 8.1 Documents Pr√©paratoires

**Benchmarks pr√©c√©dents :**
- `RAPPORT_BENCHMARK_FINAL.md` : 4 mod√®les valid√©s (Llama 3.3 70B s√©lectionn√©)
- `COMPARAISON_QUALITE_PNL_LLAMA_VS_DEEPSEEK.md` : Validation PNL (0% hallucinations)
- `SELECTION_MODELE_UNIQUE_MCP.md` : Grille s√©lection (Llama 9.8/10 vs DeepSeek 5.9/10)

**Documentation technique :**
- `DOCUMENTATION_WRAPPERS_PNL.md` : Impl√©mentation PNL compl√®te (groq_wrapper.py)
- `backend/groq_wrapper.py` : Wrapper op√©rationnel avec PNL anti-hallucination

### 8.2 Infrastructure

**Endpoint LangGraph :**
- URL : http://localhost:2024
- M√©thode : POST /invoke
- Timeout : 300s par question

**Vector DB :**
- Weaviate v4 Cloud
- 15,061 documents LangChain index√©s
- Embeddings : OpenAI text-embedding-3-small

**Wrappers mod√®les :**
- Sonnet 4.5 : Natif LangChain (`langchain-anthropic`)
- Llama 3.3 70B : Custom wrapper + PNL (`backend/groq_wrapper.py`)

---

## 9. Conclusion M√©thodologique

Ce benchmark HERO vs PRAGMATIC repr√©sente le **test d√©cisif** pour s√©lectionner le mod√®le du serveur MCP.

**Enjeu :** D√©terminer si un mod√®le 153x moins cher et 6x plus rapide peut atteindre une qualit√© acceptable (‚â•4/5) pour remplacer le champion absolu.

**Approche rigoureuse :**
- ‚úÖ 6 questions gradu√©es (couverture compl√®te complexit√© usage r√©el)
- ‚úÖ Questions na√Øves (pas d'induction r√©ponse)
- ‚úÖ Benchmarks parall√®les (infrastructure identique)
- ‚úÖ Analyse qualitative par sous-agent (objectivit√© maximale)

**D√©cision finale d√©pendra de :** Qualit√© Llama 3.3 70B sur questions complexes/ultra-complexes (Q5-Q6).

---

**Document g√©n√©r√© le 3 octobre 2025**
**Co-authored-by: St√©phane Wootha Richard <stephane@sawup.fr>**
ü§ñ M√©thodologie par Claude Code
