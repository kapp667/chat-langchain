# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Chat LangChain is a production-ready chatbot focused on question answering over LangChain documentation. It uses LangGraph for intelligent retrieval and response generation with a multi-agent research system.

### SawUp Context (October 2025)

This fork is being used for two purposes:
1. **Development Tool**: Making LangChain documentation queryable via Claude Code for development assistance
   - **Interface**: MCP (Model Context Protocol) server between backend and Claude Code ‚úÖ **OPERATIONAL**
   - **Goal**: Fast, standardized responses to LangChain coding questions during development
   - **Status**: Fully deployed and tested (October 1, 2025)

2. **Enterprise PoC**: Testing the chat-langchain architecture for an internal SawUp knowledge base system
   - **Interface**: Custom SawUp frontend (not the included Next.js frontend)
   - **Goal**: Enterprise knowledge base with multi-user access
   - **Frontend**: Will be developed separately

**Key Requirements for SawUp**:
- Must support remote/cloud vector database (for enterprise multi-user access)
- Backend must be API-first and frontend-agnostic
- Must maintain production-grade quality and reliability
- Must be upgradeable to latest LangChain versions for long-term maintenance

**Architecture Focus**:
- **Priority 1**: Backend stability and API reliability (Python/LangChain/Vector DB) ‚úÖ **COMPLETED**
- **Priority 2**: MCP integration for Claude Code (development use case) ‚úÖ **COMPLETED**
- **Priority 3**: API endpoints compatible with custom frontends (enterprise use case)
- **Not a priority**: Included Next.js frontend (will be replaced)

## Essential Commands

### Backend (Python/Poetry)
```bash
# Install dependencies
poetry install

# Run LangGraph dev server (required for MCP)
langgraph dev --no-browser --port 2024

# Run ingestion pipeline
PYTHONPATH=. poetry run python backend/ingest.py

# Force re-indexing
FORCE_UPDATE=true PYTHONPATH=. poetry run python backend/ingest.py

# Run linting
make lint

# Format code
make format

# Run tests (evaluation tests)
poetry run pytest backend/tests/evals
```

### MCP Server (Claude Code Integration)
```bash
# Install MCP server dependencies
cd mcp_server
uv pip install mcp langgraph-sdk

# Test MCP server locally
uv run --no-project python test_mcp.py

# Verify model configuration
uv run --no-project python test_model_verification.py
```

### Frontend (Next.js/Yarn)
```bash
# Navigate to frontend
cd frontend/

# Install dependencies
yarn install

# Development server
yarn dev

# Build production
yarn build
```

## Architecture Overview

### Current Deployment Stack (100% Self-Hosted)

**Application Layer:**
- `langgraph dev` (localhost:2024) - LangGraph application server
- MCP server (`mcp_server/langchain_expert.py`) - Claude Desktop integration

**Data Layer:**
- PostgreSQL (localhost:5432) - Checkpoints + Record Manager (15,061 documents tracked)
- Weaviate (localhost:8088) - Vector store (15,061 vectors with OpenAI embeddings)
- Redis (localhost:6379) - Streaming pub/sub

**External APIs:**
- OpenAI API - GPT-5 mini LLM + text-embedding-3-small
- LangSmith (optional) - Prompts are static files, tracing disabled

**Cost**: $20-50/month (OpenAI usage only) vs $285-385/month for full cloud stack

### Backend Structure (`/backend`)
The backend uses a **LangGraph-based retrieval system** with these key components:

1. **Retrieval Graph** (`backend/retrieval_graph/`):
   - `graph.py`: Main LangGraph workflow definition
   - `state.py`: State management for conversation context
   - `configuration.py`: Model configuration (GPT-5 mini defaults)
   - `prompts.py`: Loads static prompts from `prompts_static/`
   - `researcher_graph/`: Sub-graph for multi-step research planning

2. **Ingestion Pipeline** (`backend/ingest.py`):
   - Auto-detects local vs cloud Weaviate
   - Uses RecursiveURLLoader and SitemapLoader
   - Stores embeddings in Weaviate with Record Manager tracking
   - 15,061 documents indexed from LangChain/LangGraph documentation

3. **Prompts** (`backend/prompts_static/`):
   - 6 static prompt files (router, queries, research, response, etc.)
   - Zero runtime dependency on LangSmith Hub
   - Update script available for syncing from LangSmith Hub
   - Git-tracked for version control

4. **MCP Integration** (`mcp_server/`):
   - `langchain_expert.py`: MCP server with 5 tools
   - Connects to `langgraph dev` via LangGraph SDK
   - Thread and session management
   - Configuration for Claude Desktop

### Frontend Structure (`/frontend`)
Next.js application with:
- Chat UI components using @assistant-ui/react
- Real-time streaming support via LangGraph SDK
- Chakra UI for styling
- GPT-5 model selection dropdown

### LangGraph Local Development
- Configured via `langgraph.json`
- Entry point: `backend/retrieval_graph/graph.py:graph`
- Runs locally with `langgraph dev` (no cloud account needed)
- PostgreSQL + Redis for state management

## Key Technical Decisions

1. **Vector Store**: Weaviate (local Docker or cloud). Auto-detection in `ingest.py` and `retrieval.py`.

2. **LLM Configuration**: **GPT-5 mini** as default (`openai/gpt-5-mini-2025-08-07`)
   - Fast, cost-effective (95% cheaper than full GPT-5)
   - Configured in `backend/retrieval_graph/configuration.py`
   - Temperature=1 for GPT-5 (required), 0 for other models

3. **Research Flow**: Multi-agent research system
   - AI-based 3-way routing (langchain/more-info/general)
   - Multi-query generation (3-5 diverse queries per question)
   - Parallel document retrieval (10-30 documents)
   - Research planning with step-by-step execution
   - Response generation with citations

4. **State Management**: LangGraph checkpoints in PostgreSQL
   - Conversation context preserved across turns
   - Time-travel and human-in-the-loop supported
   - Record Manager tracks document indexing

5. **MCP Architecture**: Direct LangGraph SDK integration
   - FastMCP server exposing 5 tools to Claude Code
   - Thread creation via LangGraph API
   - Streaming responses with configurable timeout (180s)
   - Session caching for multi-turn conversations

## Environment Variables

Configuration in `.env` file:

```bash
# Local Docker Stack (self-hosted)
WEAVIATE_URL=http://localhost:8088
RECORD_MANAGER_DB_URL=postgresql://postgres:password@localhost:5432/chat_langchain

# Required APIs
OPENAI_API_KEY=sk-proj-...           # LLM + Embeddings
LANGCHAIN_API_KEY=lsv2_pt_...        # Optional (tracing disabled)
LANGCHAIN_TRACING_V2=false           # Disabled (free tier limitations)

# Auto-detected by langgraph dev
# REDIS_URL=redis://localhost:6379
```

## Testing Strategy

- Evaluation tests in `backend/tests/evals/`
- MCP server tests in `mcp_server/test_mcp.py`
- Model verification in `mcp_server/test_model_verification.py`
- Integration testing through graph execution

## Important Notes

1. ‚úÖ **Master branch is 100% self-hostable** despite README marketing claims
2. ‚úÖ **langgraph dev** runs locally without LangGraph Cloud account
3. ‚úÖ **MCP server** operational and integrated with Claude Desktop
4. ‚ö†Ô∏è **GPT-5 mini** is the default model (cost-effective, excellent quality)
5. ‚úÖ **Static prompts** eliminate LangSmith Hub runtime dependency

## Critical Information: Branch Status & Strategy

**‚ö†Ô∏è CRITICAL: The `langserve` branch is OBSOLETE and ABANDONED**

- Last commit: May 28, 2024 (16 months ago)
- Uses deprecated dependencies (LangChain 0.1.x, Pydantic 1.10, Weaviate v3)
- LangServe EOL: October 2025

**‚úÖ USING MASTER BRANCH (100% self-hosted)**

- Active maintenance, latest commit September 22, 2025
- Superior quality: 5/5 vs langserve 2-3/5 on complex questions
- Zero cloud coupling (Docker + PostgreSQL + Redis + Weaviate)
- Full feature parity with cloud deployment

See `ANALYSE_LANGSERVE_VS_MASTER.md` for detailed comparison.

## Known Issues & Solutions

### Issue 1: LangSmith API 403 Forbidden (RESOLVED)

**Solution: Disable LangSmith Tracing**
- Set `LANGCHAIN_TRACING_V2=false` in `.env`
- Prompts now static files in `backend/prompts_static/`
- System functions perfectly without LangSmith

### Issue 2: GPT-5 Temperature Requirement (RESOLVED)

**Problem:** GPT-5 only supports `temperature=1`

**Solution:** `backend/utils.py` line 80:
```python
temperature = 1 if model.startswith("gpt-5") else 0
```

### Issue 3: Weaviate Cloud Compatibility (RESOLVED)

**Solution:** Auto-detection in `ingest.py` and `retrieval.py`:
```python
is_local = "localhost" in WEAVIATE_URL or "127.0.0.1" in WEAVIATE_URL
if is_local:
    weaviate_client = weaviate.connect_to_local(host=host, port=port)
else:
    weaviate_client = weaviate.connect_to_weaviate_cloud(...)
```

### Issue 4: LangGraph API - Correct Invocation Pattern (CRITICAL)

**Problem:** LangGraph server does NOT expose a REST `/invoke` endpoint

**Symptoms:**
```
Client error '404 Not Found' for url 'http://localhost:2024/invoke'
```

**Root Cause:**
- LangGraph server uses SDK-based API, not direct HTTP POST
- Attempting `httpx.post(f"{url}/invoke", json=...)` will fail with 404

**‚ùå WRONG Pattern (will fail):**
```python
import httpx

async with httpx.AsyncClient() as client:
    response = await client.post(
        f"{LANGGRAPH_URL}/invoke",  # ‚Üê This endpoint doesn't exist!
        json={"input": {...}, "config": {...}}
    )
```

**‚úÖ CORRECT Pattern:**
```python
from langgraph_sdk import get_client

# Initialize client
client = get_client(url=LANGGRAPH_URL)

# Create thread
thread = await client.threads.create()
thread_id = thread["thread_id"]

# Prepare input
input_data = {
    "messages": [
        {"role": "user", "content": question}
    ]
}

# Configure model
config = {
    "configurable": {
        "query_model": model_id,
        "response_model": model_id
    }
}

# Stream response
response_text = ""
last_messages = []
chunk_count = 0

async for chunk in client.runs.stream(
    thread_id,
    ASSISTANT_ID,  # e.g., "chat"
    input=input_data,
    config=config,
    stream_mode="messages"
):
    chunk_count += 1

    # Extract messages
    if hasattr(chunk, "data") and chunk.data:
        if isinstance(chunk.data, list):
            last_messages = chunk.data
        elif isinstance(chunk.data, dict) and "messages" in chunk.data:
            last_messages = chunk.data["messages"]

# Extract final response
if last_messages:
    for msg in reversed(last_messages):
        if isinstance(msg, dict):
            if msg.get("type") == "ai" or msg.get("role") == "assistant":
                response_text = msg.get("content", "")
                break
        elif hasattr(msg, "type") and msg.type == "ai":
            response_text = msg.content
            break
```

**Key Points:**
1. **Always use `langgraph_sdk.get_client()`** - not httpx/requests
2. **Create a thread first** - `client.threads.create()`
3. **Stream with `client.runs.stream()`** - provides real-time updates
4. **Extract from chunk.data** - last AI message contains response

**Reference Implementation:**
- See `mcp_server/benchmark_models.py` lines 136-207 for complete working example
- See `mcp_server/benchmark_hero_vs_pragmatic.py` for correct pattern after fix

**Historical Note (October 3, 2025):**
- Initial `benchmark_hero_vs_pragmatic.py` incorrectly used POST `/invoke`
- All 6 tests failed with 404 errors
- Fixed by switching to LangGraph SDK pattern ‚Üí 100% success rate
- Documented to prevent future regression

## Investigation History & Decisions

### October 1, 2025: MCP Server Implementation and Integration ‚úÖ **COMPLETED**

**Context:** Implemented and deployed MCP (Model Context Protocol) server to integrate LangGraph backend with Claude Desktop for direct querying during development.

**Strategic Decision:**
- Chose master branch self-hosted over langserve migration
- Rationale: 5/5 quality vs 2-3/5, minimal effort (4-6h), LangChain-maintained
- ROI: 20x better than custom RAG (time √ó quality √ó maintenance)

**Infrastructure Setup:**

**Docker Stack (local):**
```bash
docker run -d --name postgres -p 5432:5432 \
  -e POSTGRES_PASSWORD=password postgres:16

docker run -d --name weaviate -p 8088:8080 \
  -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true \
  cr.weaviate.io/semitechnologies/weaviate:1.32.0

docker run -d --name redis -p 6379:6379 redis:7
```

**Ingestion:**
```bash
# Standard ingestion (15,061 documents)
PYTHONPATH=. poetry run python backend/ingest.py

# Results:
# - 15,061 vectors in Weaviate (OpenAI text-embedding-3-small)
# - 15,061 records in PostgreSQL Record Manager
# - Collection: LangChain_General_Guides_And_Tutorials_OpenAI_text_embedding_3_small
```

**LangGraph Dev Server:**
```bash
langgraph dev --no-browser --port 2024

# Endpoints:
# - POST /threads - Create conversation thread
# - POST /threads/{id}/runs/stream - Execute graph with streaming
# - GET /threads/{id}/state - Get conversation state
```

**MCP Server Implementation:**

**File: `mcp_server/langchain_expert.py` (309 lines)**

Created MCP server with 5 tools:

1. **`ask_langchain_expert`** - Main query tool
   - Default model: `openai/gpt-5-mini-2025-08-07`
   - Configurable timeout: 180s (allows model initialization)
   - Session management for multi-turn conversations
   - Returns citations with numbered references

2. **`ask_langchain_expert_advanced`** - Complex queries
   - Uses full GPT-5 (`openai/gpt-5-2025-08-07`)
   - Extended timeout: 180s
   - For architecture/design questions

3. **`check_langchain_expert_status`** - System health check
   - Verifies LangGraph server connectivity
   - Lists available models
   - Reports active sessions and indexed documents

4. **`list_sessions`** - View active conversations
   - Shows session IDs and thread mappings

5. **`clear_session`** - Reset conversation context
   - Clears thread cache for session ID

**Key Implementation Details:**

**Thread Management:**
```python
async def _get_or_create_thread_id(session_id: Optional[str] = None) -> str:
    if session_id and session_id in _thread_cache:
        return _thread_cache[session_id]

    # Create thread via LangGraph API (not local UUID)
    client = get_client(url=LANGGRAPH_URL)
    thread = await client.threads.create()
    thread_id = thread["thread_id"]

    if session_id:
        _thread_cache[session_id] = thread_id

    return thread_id
```

**Streaming Implementation:**
```python
async for chunk in client.runs.stream(
    thread_id,
    ASSISTANT_ID,
    input=input_data,
    config=config,
    stream_mode="messages"
):
    if hasattr(chunk, "data") and chunk.data:
        if isinstance(chunk.data, list):
            last_messages = chunk.data
        elif isinstance(chunk.data, dict) and "messages" in chunk.data:
            last_messages = chunk.data["messages"]
```

**Claude Desktop Integration:**

**File: `~/Library/Application Support/Claude/claude_desktop_config.json`**

```json
{
  "mcpServers": {
    "youtube": {
      "command": "npx",
      "args": ["-y", "@coyasong/youtube-mcp-server"],
      "env": {"YOUTUBE_API_KEY": "..."}
    },
    "langchain-expert": {
      "command": "uv",
      "args": [
        "--directory",
        "/Users/stephane/Documents/work/chat-langchain/mcp_server",
        "run",
        "--no-project",
        "python",
        "langchain_expert.py"
      ],
      "env": {
        "LANGGRAPH_URL": "http://localhost:2024"
      }
    }
  },
  "globalShortcut": "Ctrl+Cmd+Space"
}
```

**Configuration coexists with existing MCP servers (youtube).**

**Testing and Validation:**

**Test Script: `mcp_server/test_mcp.py`**

Three test scenarios validated:
1. **System status check** - Verified connectivity
2. **Simple question** - "What is LangChain?"
3. **Complex question** - "How do LangGraph checkpoints work with PostgreSQL?"

**Results:**
- ‚úÖ Thread creation successful (HTTP 200)
- ‚úÖ Streaming operational
- ‚úÖ Complex question answered with citations
- ‚úÖ Response quality: **Excellent** (comprehensive, structured, cited)
- ‚úÖ Response time: ~8-20 seconds (after initialization)

**Example Response Quality:**

Question: "How do LangGraph checkpoints work with PostgreSQL?"

Response included:
- Conceptual explanation (checkpoints, threads, capabilities)
- Checkpointer interface and implementations
- Practical setup steps with PostgresS aver
- State inspection and time-travel
- Serialization and encryption notes
- Quick start checklist
- 20+ citations with links to documentation

**Model Configuration:**

**File: `backend/retrieval_graph/configuration.py`**

Changed defaults from GPT-5 to GPT-5 mini:
```python
query_model: str = field(
    default="openai/gpt-5-mini-2025-08-07",
    metadata={"description": "Model for processing queries"}
)

response_model: str = field(
    default="openai/gpt-5-mini-2025-08-07",
    metadata={"description": "Model for generating responses"}
)
```

**Hot-reload verified:**
```
WatchFiles detected changes in 'backend/retrieval_graph/configuration.py'. Reloading...
```

**Documentation Created:**

1. **`mcp_server/README.md`** (298 lines)
   - Installation and configuration
   - Tool descriptions and usage examples
   - Model selection strategy
   - Performance benchmarks
   - Troubleshooting guide

2. **`mcp_server/QUICK_START.md`**
   - Rapid testing guide
   - Verification commands
   - Expected results

3. **`mcp_server/MANUAL_START.md`**
   - Complete manual startup guide
   - Daily usage workflow
   - Monitoring and debugging
   - Useful commands cheatsheet

4. **`mcp_server/STATUS.md`**
   - Project status snapshot
   - Configuration summary
   - Example usage patterns

5. **`mcp_server/test_mcp.py`** - Automated test suite
6. **`mcp_server/test_model_verification.py`** - Model validation

**Deployment Status:**

**‚úÖ FULLY OPERATIONAL - Production Ready**

**MCP Server:**
- ‚úÖ Configured in Claude Desktop
- ‚úÖ 5 tools accessible from Claude Code
- ‚úÖ Thread and session management working
- ‚úÖ Streaming responses operational
- ‚úÖ GPT-5 mini as default (cost-effective)

**Backend:**
- ‚úÖ langgraph dev running (localhost:2024)
- ‚úÖ 15,061 documents indexed
- ‚úÖ PostgreSQL + Weaviate + Redis operational
- ‚úÖ GPT-5 mini configured and validated

**Usage Example (Claude Code):**
```
Utilise check_langchain_expert_status pour v√©rifier le syst√®me.

Utilise langchain_expert pour expliquer les checkpoints LangGraph.

Utilise langchain_expert pour expliquer comment impl√©menter les
checkpoints avec PostgreSQL (session "debug-checkpoints").
```

**Performance Metrics:**

- **First query**: ~180s (model initialization)
- **Subsequent queries**: 8-20 seconds
- **Quality**: 5/5 (expert-level with citations)
- **Cost**: ~$0.10/1M tokens (GPT-5 mini) vs ~$2/1M (full GPT-5)
- **Documents retrieved**: 10-30 per query (adaptive)
- **Resource usage**: ~500 MB RAM (Docker stack + langgraph dev)

**Key Architectural Insights:**

1. **Direct SDK Integration**: MCP server uses LangGraph SDK, not HTTP proxy
2. **Thread Management Required**: Must call `client.threads.create()` before streaming
3. **API Signature**: `client.runs.stream(thread_id, assistant_id, options)` (positional args)
4. **Frontend Pattern**: Next.js acts as proxy (`/api/*` ‚Üí `localhost:2024/*`)
5. **MCP Pattern**: Direct connection without proxy layer

**Lessons Learned:**

1. LangGraph SDK requires explicit thread creation via API (not local UUID)
2. FastMCP simplifies tool implementation with decorators and type hints
3. Session management critical for multi-turn conversations
4. GPT-5 mini provides excellent quality at 95% cost reduction
5. Hot-reload in langgraph dev enables zero-downtime config changes
6. MCP server and existing MCP servers (youtube) coexist seamlessly

**Success Criteria Achieved:**

- ‚úÖ MCP server operational in Claude Desktop
- ‚úÖ All 5 tools accessible and tested
- ‚úÖ Response quality matches production standards
- ‚úÖ GPT-5 mini configured as default
- ‚úÖ Comprehensive documentation created
- ‚úÖ Manual startup guide for daily use
- ‚úÖ Zero cloud dependencies (except OpenAI API)

**Next Steps for Daily Use:**

1. **Start backend** (if not running):
   ```bash
   cd /Users/stephane/Documents/work/chat-langchain
   langgraph dev --no-browser --port 2024
   ```

2. **Verify status** in Claude Code:
   ```
   Utilise check_langchain_expert_status
   ```

3. **Query LangChain expert**:
   ```
   Utilise langchain_expert pour [your question]
   ```

See `mcp_server/MANUAL_START.md` for complete usage guide.

**Cost Comparison (Monthly):**
- **Local stack**: $0 (Docker containers)
- **OpenAI API**: $20-50 (GPT-5 mini usage)
- **Total**: $20-50/month
- **vs Cloud**: $285-385/month (90% cost reduction)

**Quality Validation:**

Tested with complex question about PostgreSQL checkpoints:
- ‚úÖ Multi-document synthesis (20+ sources)
- ‚úÖ Structured response (concepts ‚Üí practical ‚Üí examples)
- ‚úÖ Numbered citations with links
- ‚úÖ Code snippets included
- ‚úÖ Follow-up question suggestions

**Deployment Type:** Development tool (manual start)
**Rationale:** Maintaining control during learning phase, avoiding automatic startup until fully mastered

---

### Earlier Investigations (September-October 2025)

For complete investigation history including:
- September 30, 2025: Initial Setup & Weaviate Cloud Compatibility
- September 30, 2025: Branch Strategy Investigation
- September 30, 2025: Master Branch Self-Hosting Deep Dive
- October 1, 2025: Master Branch Local Setup Completed
- October 1, 2025: Static Prompts Implementation
- October 1, 2025: GPT-5 Integration and API Testing Guide

See sections above in this file (lines 389-1262).

---

## Current Project State (October 1, 2025)

**‚úÖ FULLY OPERATIONAL - Production Ready**

**Infrastructure:**
- ‚úÖ Docker stack (PostgreSQL + Weaviate + Redis) running locally
- ‚úÖ langgraph dev (localhost:2024) operational
- ‚úÖ MCP server configured in Claude Desktop
- ‚úÖ 15,061 documents indexed with GPT-5 mini

**Model Configuration:**
- **Default**: `openai/gpt-5-mini-2025-08-07` (query + response)
- **Advanced**: `openai/gpt-5-2025-08-07` (via `ask_langchain_expert_advanced`)
- **DeepSeek**: **MUST use `deepseek-reasoner` V3.2-Exp** (not deepseek-chat)
  - Wrapper enforces `deepseek-reasoner` regardless of config
  - Reason: 8x higher output limits (64K vs 8K), superior reasoning
  - See `backend/deepseek_wrapper.py` for implementation
- **Temperature**: 1 for GPT-5, 0 for others (auto-detected)

**Cost**: $20-50/month (OpenAI usage only)
**Quality**: 5/5 (state-of-the-art, proven by chat.langchain.com)
**Maintenance**: LangChain team (master branch active)

**Key Files:**
- `backend/retrieval_graph/configuration.py` - Model defaults (GPT-5 mini)
- `backend/prompts_static/` - Static prompts (zero runtime dependency)
- `backend/ingest.py` - Auto-detect local/cloud Weaviate
- `backend/retrieval.py` - Auto-detect local/cloud Weaviate
- `mcp_server/langchain_expert.py` - MCP server (5 tools)
- `~/Library/Application Support/Claude/claude_desktop_config.json` - MCP config

**Documentation:**
- `MANUAL_START.md` - Daily usage guide
- `STACK_LOCAL_EXPLAINED.md` - Architecture deep dive
- `LANGGRAPH_DEV_VS_CLOUD.md` - Operational comparison
- `mcp_server/README.md` - MCP server documentation
- `mcp_server/STATUS.md` - Current state snapshot

**For New Contributors:**
1. Read `STACK_LOCAL_EXPLAINED.md` for architecture understanding
2. Read `MANUAL_START.md` for daily workflow
3. Use MCP tools in Claude Code for LangChain questions during development
4. Model is GPT-5 mini by default (cost-effective, excellent quality)

---

## Common Patterns & Solutions (October 2025)

This section documents recurring technical patterns and solutions discovered during development. These patterns are likely to be relevant for future work.

### Pattern 1: Anthropic Models Require Explicit `max_tokens`

**Problem**: Claude models (Anthropic provider) truncate responses unexpectedly, even for simple questions.

**Symptom**:
- Response cuts off mid-sentence (e.g., "async def research(\n        self, ")
- Response length significantly shorter than expected (3.5k chars vs 14k chars for same question)
- Chunk count very low (78 chunks vs 3,250 chunks for comparable response)

**Root Cause**:
Anthropic models have **no default `max_tokens`** value. If not specified, they use a very conservative limit that causes truncation.

**Solution**:
Add explicit `max_tokens` parameter when loading Anthropic models:

```python
# File: backend/utils.py (load_chat_model function)

def load_chat_model(fully_specified_name: str) -> BaseChatModel:
    """Load a chat model from a fully specified name."""
    if "/" in fully_specified_name:
        provider, model = fully_specified_name.split("/", maxsplit=1)
    else:
        provider = ""
        model = fully_specified_name

    model_kwargs = {"temperature": temperature, "stream_usage": True}

    # Anthropic models require explicit max_tokens for long responses
    if provider == "anthropic":
        model_kwargs["max_tokens"] = 16384  # Support long, detailed responses

    return init_chat_model(model, model_provider=provider, **model_kwargs)
```

**Key Insight**:
- OpenAI models: Have reasonable defaults (don't need explicit `max_tokens`)
- Anthropic models: **MUST** specify `max_tokens` (no default)
- DeepSeek models: Have defaults (similar to OpenAI)

**When to Apply**: Any time you're adding support for a new Anthropic model (Claude Sonnet, Claude Opus, etc.)

### Pattern 2: DeepSeek Requires Explicit JSON Mode for Structured Output

**Problem**: DeepSeek models return empty responses or fail silently when using LangChain's generic `with_structured_output()` method.

**Symptom**:
- HTTP 200 OK but empty response content
- Error: `TypeError: 'NoneType' object is not subscriptable`
- No error message from the API

**Root Cause**:
DeepSeek API requires explicit `response_format={'type': 'json_object'}` parameter, which LangChain's generic `with_structured_output()` doesn't provide.

**CRITICAL: Project requires `deepseek-reasoner` (V3.2-Exp Thinking Mode)**

Our wrapper ALWAYS uses `deepseek-reasoner` instead of `deepseek-chat` because:
- **8x higher output limits**: 64K max vs 8K for deepseek-chat
- **4x higher default**: 32K vs 4K for deepseek-chat
- **Superior reasoning**: Thinking mode provides better quality for complex questions
- **Cost**: Slightly higher but justified by output quality ($1.40/M output vs $0.42/M)

**Solution**:
Create a custom wrapper that:
1. Sets explicit JSON mode: `response_format={'type': 'json_object'}`
2. Enhances prompts with JSON instructions and examples
3. Extracts JSON from markdown code blocks (fallback)

```python
# File: backend/deepseek_wrapper.py

from langchain_deepseek import ChatDeepSeek

def load_deepseek_for_structured_output(
    model_name: str,
    schema: Optional[Type[BaseModel]] = None
) -> BaseChatModel:
    """Load DeepSeek model configured for structured output.

    ALWAYS uses deepseek-reasoner (V3.2-Exp) regardless of model_name parameter.
    """
    # ALWAYS use deepseek-reasoner (project requirement)
    # Provides 8x higher output limits (64K vs 8K) and superior reasoning
    model_id = "deepseek-reasoner"

    # CRITICAL: Explicit JSON mode + maximum tokens for DeepSeek
    model = ChatDeepSeek(
        model=model_id,
        response_format={'type': 'json_object'},  # Required for DeepSeek
        temperature=0,
        max_tokens=64000  # deepseek-reasoner max (vs 8K for deepseek-chat)
    )

    return model
```

**Integration Pattern**:
Detect DeepSeek in your code and use the custom wrapper:

```python
# File: backend/retrieval_graph/researcher_graph/graph.py

from backend.deepseek_wrapper import generate_queries_deepseek

async def generate_queries(state: ResearcherState, *, config: RunnableConfig):
    configuration = AgentConfiguration.from_runnable_config(config)

    # Special handling for DeepSeek models
    if "deepseek" in configuration.query_model.lower():
        messages = [
            {"role": "system", "content": configuration.generate_queries_system_prompt},
            {"role": "human", "content": state.question},
        ]
        try:
            response = await generate_queries_deepseek(
                messages,
                configuration.query_model,
                Response  # Pydantic schema
            )
            return {"queries": response["queries"]}
        except Exception as e:
            # Fallback: return original question as single query
            print(f"DeepSeek query generation failed: {e}")
            return {"queries": [state.question]}

    # Standard logic for other models (OpenAI, Anthropic, etc.)
    model = load_chat_model(configuration.query_model).with_structured_output(Response)
    # ... rest of standard logic
```

**Key Components**:
1. **Prompt Enhancement**: Add explicit JSON format instructions with examples
2. **Explicit JSON Mode**: `response_format={'type': 'json_object'}` parameter
3. **Fallback Extraction**: Extract JSON from markdown ```json``` blocks if needed
4. **Error Handling**: Graceful fallback to single-query if JSON parsing fails

**When to Apply**: Any time you need structured JSON output from DeepSeek models

### Pattern 3: LangGraph Dev Requires Poetry Environment

**Problem**: Running `langgraph dev` directly fails with package or Python version errors.

**Symptom**:
```
Error: Required package 'langgraph-api' is not installed.
Note: The in-mem server requires Python 3.11 or higher to be installed.
You are currently using Python 3.9.
```

**Root Cause**:
System Python (e.g., `/usr/bin/python3` = Python 3.9) doesn't have the Poetry virtual environment packages. `langgraph dev` picks up system Python instead of the virtualenv Python.

**Solution**:
Always prefix with `poetry run`:

```bash
# ‚ùå WRONG - Uses system Python
langgraph dev

# ‚úÖ CORRECT - Uses Poetry virtualenv Python 3.11
poetry run langgraph dev
```

**Background Task Pattern**:
When running in background, ensure Poetry context:

```bash
# Launch with proper environment and log to file
poetry run langgraph dev > /tmp/langgraph_dev.log 2>&1 &
LANGGRAPH_PID=$!
echo "LangGraph dev started (PID: $LANGGRAPH_PID)"
```

**Verification**:
Check which Python is being used:

```bash
# System Python (wrong)
which python3
# /usr/bin/python3 (Python 3.9)

# Poetry virtualenv Python (correct)
poetry run which python
# /Users/.../pypoetry/virtualenvs/chat-langchain-xxx-py3.11/bin/python
```

**When to Apply**: Any Poetry project where you need to run CLI tools installed as project dependencies

### Pattern 4: Debugging Truncation with Chunk Count Analysis

**Problem**: Need to quickly determine if a response is truncated without reading the entire content.

**Solution**: Use **chunk count** as a diagnostic metric.

**Pattern**:
```python
# During streaming response collection
chunk_count = 0
async for chunk in client.runs.stream(...):
    chunk_count += 1
    # ... process chunk

print(f"Chunks received: {chunk_count}")
```

**Interpretation**:
- **Low chunk count** (< 100): Likely truncated or very short response
- **Normal chunk count** (1,000-3,000): Full response for complex questions
- **High chunk count** (> 5,000): Very detailed response

**Example from Benchmarking**:
| Model | Response Length | Chunk Count | Status |
|-------|----------------|-------------|---------|
| GPT-5 Mini | 14,326 chars | 3,250 chunks | ‚úÖ Complete |
| Claude (broken) | 3,460 chars | 78 chunks | ‚ùå Truncated |
| Claude (fixed) | ~14,000 chars | ~3,000 chunks | ‚úÖ Complete |

**When to Apply**: Diagnosing response quality issues, comparing model outputs, benchmarking

### Pattern 5: LangGraph SDK Connection Testing

**Problem**: Benchmark scripts fail with "Cannot connect to LangGraph" despite server running.

**Quick Test Pattern**:
```python
# File: test_connection.py
import asyncio
from langgraph_sdk import get_client

async def test():
    try:
        client = get_client(url='http://localhost:2024')
        assistants = await client.assistants.search()
        print(f'‚úÖ Connected! Found {len(assistants)} assistants')
        for a in assistants[:2]:
            print(f'  - {a.get("name")} (graph: {a.get("graph_id")})')
    except Exception as e:
        print(f'‚ùå Connection failed: {type(e).__name__}: {str(e)}')
        import traceback
        traceback.print_exc()

asyncio.run(test())
```

**Common Issues**:
1. **Wrong Python environment**: Use `poetry run python test_connection.py`
2. **Wrong URL**: Should be `http://localhost:2024` (not `http://127.0.0.1:2024` for SDK)
3. **Server not started**: Check `ps aux | grep "langgraph dev"`
4. **Port conflict**: Check `lsof -nP -iTCP:2024`

**Manual curl Test**:
```bash
# Test health endpoint (will 404, but proves server responds)
curl http://127.0.0.1:2024/health

# Test assistants search (should return JSON)
curl -s http://127.0.0.1:2024/assistants/search -X POST -H "Content-Type: application/json" -d '{}'
```

**When to Apply**: Debugging connection issues between client code and LangGraph server

### Pattern 6: Provider-Specific Integration in `graph.py`

**Problem**: Newer LLM providers (DeepSeek, Groq) not supported by LangChain's `init_chat_model()` or have specific requirements for structured outputs.

**Solution**: Add provider detection in `backend/retrieval_graph/researcher_graph/graph.py`:

```python
# Location: backend/retrieval_graph/researcher_graph/graph.py
# Function: generate_queries()

async def generate_queries(state: ResearcherState, *, config: RunnableConfig):
    """Generate search queries based on the question."""

    class Response(TypedDict):
        queries: list[str]

    configuration = AgentConfiguration.from_runnable_config(config)

    # Special handling for DeepSeek models (require explicit JSON mode)
    if "deepseek" in configuration.query_model.lower():
        messages = [
            {"role": "system", "content": configuration.generate_queries_system_prompt},
            {"role": "human", "content": state.question},
        ]
        try:
            response = await generate_queries_deepseek(
                messages,
                configuration.query_model,
                Response
            )
            return {"queries": response["queries"]}
        except Exception as e:
            # Fallback: return original question as single query
            print(f"DeepSeek query generation failed: {e}")
            return {"queries": [state.question]}

    # Standard logic for other models (OpenAI, Anthropic, Google, Groq with wrapper)
    structured_output_kwargs = (
        {"method": "function_calling"} if "openai" in configuration.query_model else {}
    )
    model = load_chat_model(configuration.query_model).with_structured_output(
        Response, **structured_output_kwargs
    )
    # ... rest of standard logic
```

**Modifications Made (October 2, 2025)**:

1. **Line 19**: Added import for DeepSeek wrapper functions
2. **Lines 42-58**: Added DeepSeek-specific branch with custom wrapper call
3. **Provider detection logic**: Uses `configuration.query_model.lower()` to check for "deepseek"

**Files Modified**:
- `backend/retrieval_graph/researcher_graph/graph.py`
- `backend/utils.py` (Groq detection added at line 79-82)
- `backend/deepseek_wrapper.py` (created)
- `backend/groq_wrapper.py` (created)

**When to Review/Refactor**:

‚ö†Ô∏è **These modifications may become obsolete when:**
1. LangChain officially supports DeepSeek/Groq in `init_chat_model()`
2. DeepSeek implements `json_schema` mode (currently only supports `json_object`)
3. LangGraph introduces provider-agnostic structured output handling

**Maintenance Notes**:
- Check LangChain changelogs for DeepSeek/Groq integration updates
- If `init_chat_model()` adds native support, remove custom branches and wrappers
- Keep wrappers as reference for similar provider-specific needs
- Test after LangChain major version upgrades (0.3 ‚Üí 0.4, etc.)

**Why These Modifications Were Necessary**:
- DeepSeek: Does NOT support `json_schema` mode required by LangChain's `with_structured_output()`
- Groq: NOT recognized by `init_chat_model()` as valid provider (as of LangChain 0.3.x)
- Both require workarounds until official LangChain integration

### Pattern 7: Groq Tool Calling Limitations & JSON Mode Workaround

**Problem**: Groq models fail with `with_structured_output()` in LangGraph, despite official LangChain support via `langchain-groq`.

**Error Encountered**:
```json
{
  "error": "APIError",
  "message": "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details."
}
```

**Root Cause Analysis** (October 2, 2025):

Based on deep research into LangChain/Groq integration:

1. **Package Status**: `langchain-groq` officially exists and is maintained
   - Package: `langchain-groq` (Python) / `@langchain/groq` (JS)
   - Class: `ChatGroq` inherits from `BaseChatModel`
   - Features: Streaming ‚úì, Async ‚úì, Tool Calling ‚úì*, Structured Output ‚úì*

2. **Known Limitation**: Tool calling in Groq has **reliability issues** in LangGraph context
   - Works in simple use cases
   - Fails in multi-step graphs (like researcher_graph)
   - Error: "Failed to call a function" without detailed error

3. **Documented Workaround**: Use JSON mode instead of tool calling
   - Groq supports `response_format={'type': 'json_object'}` natively
   - Similar approach as DeepSeek (both lack `json_schema` mode)

**Solution Implemented**: Groq-specific JSON mode wrapper (`backend/groq_wrapper.py`)

```python
# backend/groq_wrapper.py
async def generate_queries_groq(messages, model_id, schema):
    """Generate queries using Groq with JSON mode workaround."""
    model = ChatGroq(
        model=model_name,
        temperature=0,
        model_kwargs={"response_format": {"type": "json_object"}}
    )

    # Enhance system prompt with JSON schema instructions
    enhanced_content = f"""{system_message}

CRITICAL: You MUST respond with valid JSON only in this exact format:
{schema_description}

Do not include any text outside the JSON structure."""

    response = await model.ainvoke(enhanced_messages)
    return json.loads(response.content)
```

**Integration in graph.py**:
```python
# backend/retrieval_graph/researcher_graph/graph.py
from backend.groq_wrapper import generate_queries_groq

async def generate_queries(state, *, config):
    # Special handling for Groq models
    if "groq" in configuration.query_model.lower():
        response = await generate_queries_groq(
            messages, configuration.query_model, Response
        )
        return {"queries": response["queries"]}
```

**Supported Groq Models** (October 2025):

| Model | Context | Status | Use Case |
|-------|---------|--------|----------|
| **llama-3.1-8b-instant** ‚≠ê | 131K | Production | Ultra-fast, recommended |
| llama-3.3-70b-versatile | 131K | Production | High quality, versatile |
| gemma2-9b-it | 8K | Deprecated ‚Üí use llama-3.1-8b-instant |
| deepseek-r1-distill-llama-70b | 131K | Preview | Reasoning (no tools) |

**Performance Results** (Benchmark Test 3 - Complex Question):

```
Groq Llama 3.1 8B Instant (with wrapper):
- Time: 8.18s (vs Claude 109s = 13x faster!)
- Chars: 4,970 (vs Claude 23,805 = 20% length)
- Chunks: 1,068 (ultra-fast streaming)
- Quality: Good (architectural overview, lacks production depth)
```

**Key Findings**:
- ‚úÖ Groq is **dramatically faster** (8s vs 109s)
- ‚úÖ JSON mode workaround **works perfectly**
- ‚ö†Ô∏è Responses are **shorter/less detailed** than Claude
- ‚ö†Ô∏è Context window OK (131K tokens - no overflow issues)

**When to Use Groq**:
- **Use for**: Simple/moderate questions requiring speed
- **Avoid for**: Complex production questions requiring deep implementation details
- **Sweet spot**: Quick lookups, concept explanations, moderate technical Q&A

**Technical Details**:
- Groq uses Language Processing Unit (LPU) infrastructure for ultra-low latency
- `reasoning_format` parameter available for DeepSeek-R1-distill model
- Batch processing available (50% cost reduction)
- **Architecture**: Groq ‚â† OpenAI (custom BaseChatModel, not BaseChatOpenAI like DeepSeek)

**When to Review/Refactor**:
‚ö†Ô∏è **Monitor for**:
1. Official LangGraph support for Groq tool calling (may fix the API error)
2. `init_chat_model()` adding "groq" as recognized provider
3. Groq improving tool calling reliability in multi-step scenarios

**Migration Path** (when Groq tool calling is fixed):
1. Remove Groq branch from `graph.py` (lines 61-78)
2. Remove `backend/groq_wrapper.py`
3. Update `backend/utils.py` to use `init_chat_model()` for Groq
4. Test thoroughly before deploying

---

## Investigation History & Decisions

### October 2, 2025: MCP Model Benchmarking & Configuration Fixes

**Context:** After successful MCP deployment (October 1), discovered Claude and DeepSeek models had issues in production testing.

**Initial Findings:**
- GPT-5 Full: Excellent quality but slow (~180-250s complex questions)
- GPT-5 Mini: Excellent quality and fast (~60-120s) ‚Üê **Baseline for comparison**
- Claude Sonnet 4.5: Incomplete responses (3,460 chars vs 14,326 expected)
- DeepSeek Chat: Empty responses despite HTTP 200 OK

**Root Causes Identified:**

**1. Claude Truncation (Configuration Error)**
- **Incorrect initial diagnosis**: Assumed Claude was incomplete "by design"
- **User challenge**: "Je suis tr√®s √©tonn√© qu'entropic-clode 4.5 et des r√©ponses coup√©es" (skeptical of truncation)
- **Actual cause**: Missing `max_tokens` parameter (Anthropic requires explicit value, no default)
- **Evidence**: Response ended mid-function: `"async def research(\n        self, "`
- **Fix**: Added `max_tokens=16384` in `backend/utils.py` for all Anthropic models

**2. DeepSeek Structured Output (API Incompatibility)**
- **Initial diagnosis**: "DeepSeek incompatible with LangChain"
- **User insight**: "Je pense que si tes conclusions sont que le mod√®le DeepSeek n'est pas compatible alors qu'il vient de sortir et que c'est probablement un mod√®le r√©volutionnaire sur le march√©, c'est certainement que tu comprends mal l'API de DeepSeek."
- **Actual cause**: DeepSeek requires `response_format={'type': 'json_object'}` which LangChain's generic `with_structured_output()` doesn't provide
- **Solution**: Created custom wrapper (`backend/deepseek_wrapper.py`) with:
  - Explicit JSON mode configuration
  - Enhanced prompts with format examples
  - Fallback JSON extraction from markdown
- **Integration**: Modified `backend/retrieval_graph/researcher_graph/graph.py` to detect and handle DeepSeek specially

**3. Python Environment Issues (Tooling)**
- **Problem**: Background benchmark tests failed with "cannot connect" despite server running
- **Cause**: Tests used system Python 3.9 instead of Poetry virtualenv Python 3.11
- **Solution**: Always use `poetry run` prefix for CLI commands in Poetry projects

**Code Changes:**
1. `backend/utils.py` (load_chat_model): Added Anthropic `max_tokens=16384`
2. `backend/deepseek_wrapper.py` (NEW): 259 lines - Complete DeepSeek JSON mode wrapper
3. `backend/retrieval_graph/researcher_graph/graph.py` (generate_queries): Added DeepSeek detection and wrapper call
4. `langgraph.json`: No changes (already compatible)

**Testing Status (October 2, 14:22 UTC):**
- ‚úÖ LangGraph dev server running (PID 27923, localhost:2024)
- üîÑ Claude Sonnet 4.5 test in progress (PID 32578, expected ~8-12 min)
- üîÑ DeepSeek Chat test in progress (PID 33332, expected ~8-12 min)
- ‚è≥ Pending: Results comparison vs GPT-5 Mini baseline
- ‚è≥ Pending: Updated benchmark report with corrected data

**Key Lessons:**
1. **Challenge initial assumptions**: User was right to question Claude truncation analysis
2. **Don't assume API incompatibility**: DeepSeek works, just needs proper configuration
3. **Empirical testing beats theoretical analysis**: Raw response data revealed true issues
4. **Model-specific quirks require model-specific code**: Not all LLMs work with generic abstractions

**Next Steps:**
- [ ] Wait for benchmark tests to complete (~10 minutes)
- [ ] Verify Claude responses are now complete (14k+ chars expected)
- [ ] Verify DeepSeek returns actual content (not empty)
- [ ] Generate updated comparison report with corrected data
- [ ] Update MCP server model recommendations if needed

**Co-authored-by: St√©phane Wootha Richard <stephane@sawup.fr>**
